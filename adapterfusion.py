# -*- coding: utf-8 -*-
"""AdapterFusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RKuLiJizjaHB1f81B1Ae5fUjWuMonZTG
"""

!pip install -q transformers datasets peft accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import torch

def train_lora(dataset, adapter_dir, text_columns, max_length=256):
    model_name = "gpt2-xl"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token  # GPT2 has no PAD token

    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.resize_token_embeddings(len(tokenizer))

    # LoRA config
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)

    # üîπ Combine text columns into one training text field
    def merge_columns(examples):
        texts = []
        for i in range(len(examples[text_columns[0]])):
            merged = ""
            for col in text_columns:
                if col in examples and examples[col][i] is not None:
                    merged += f"{col.upper()}: {examples[col][i]}\n"
            texts.append(merged.strip())
        return {"merged_text": texts}

    dataset = dataset.map(merge_columns, batched=True)

    # üîπ Tokenize with labels for CLM loss
    def tokenize_fn(examples):
        outputs = tokenizer(
            examples["merged_text"],
            truncation=True,
            padding="max_length",
            max_length=max_length,
        )
        outputs["labels"] = outputs["input_ids"].copy()
        return outputs

    tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)

    # --------------------------
    # Training setup
    # --------------------------
    training_args = TrainingArguments(
        output_dir=adapter_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        num_train_epochs=5,
        learning_rate=1e-4,
        logging_steps=50,
        save_strategy="epoch",
        report_to="none",  # disable W&B
        fp16=torch.cuda.is_available(),
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )

    trainer.train()

    model.save_pretrained(adapter_dir)
    tokenizer.save_pretrained(adapter_dir)

"""# Train on Story"""

# STORY LoRA Adapter
# --------------------------
story_ds = load_dataset("euclaise/writingprompts", split="train[:20000]")
train_lora(
    dataset=story_ds,
    adapter_dir="./lora_story",
    text_columns=["prompt", "story"]
)

"""# Train on Code"""

code_ds = load_dataset("microsoft/rStar-Coder", "synthetic_sft", split="train[:20000]")
train_lora(
    dataset=code_ds,
    adapter_dir="./lora_code",
    text_columns=["question", "seed_question", "response", "code"]
)

"""# Save to Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Save to Drive (change path if you want)
!cp -r ./lora_story /content/drive/MyDrive/AdapterFusion/lora_story
!cp -r ./lora_code /content/drive/MyDrive/AdapterFusion/lora_code
print("Models saved to Drive!!!.")

"""# Merging"""

!pip install -q transformers peft accelerate bitsandbytes

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -----------------------------
# 1Ô∏è‚É£ Load the base GPT-2-XL model
# -----------------------------
base_model_name = "gpt2-xl"
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# -----------------------------
# 2Ô∏è‚É£ Merge story adapter first
# -----------------------------
story_adapter_path = "./lora_story"
code_adapter_path  = "./lora_code"

print("üîπ Merging story adapter...")
story_model = PeftModel.from_pretrained(base_model, story_adapter_path)
story_model = story_model.merge_and_unload()  # merges LoRA into base weights

# -----------------------------
# 3Ô∏è‚É£ Now merge the code adapter onto the updated model
# -----------------------------
print("üîπ Merging code adapter...")
code_model = PeftModel.from_pretrained(story_model, code_adapter_path)
merged_model = code_model.merge_and_unload()  # merges second adapter too

# -----------------------------
# 4Ô∏è‚É£ Save the final unified model
# -----------------------------
save_path = "./gpt2-xl-lora-story-code-merged"
merged_model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"‚úÖ Merged model saved at: {save_path}")

"""# TEST"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

merged_model_path = "./gpt2-xl-lora-story-code-merged"

tokenizer = AutoTokenizer.from_pretrained(merged_model_path)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    merged_model_path,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)
model.eval()

def generate(prompt, max_length=200, temperature=0.8):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# üîπ Test prompts
print("=== STORY SAMPLE ===")
print(generate("Once upon a time in a mysterious city,"))

print("\n=== CODE SAMPLE ===")
print(generate("Write a Python function to calculate Fibonacci numbers"))

"""# Save to Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Save to Drive (change path if you want)
!cp -r ./gpt2-xl-lora-story-code-merged/ /content/drive/MyDrive/AdapterFusion/gpt2-xl-lora-story-code-merged
print("Model saved to Drive!!!.")

